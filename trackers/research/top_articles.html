<!DOCTYPE html>
<html>
<head>
<title>Top Research Articles of the Week</title>
</head>
<body>
<h1>Top Research Articles of the Week</h1>

<h2>Title: ArcMMLU: A Library and Information Science Benchmark for Large Language Models</h2>
<h3>Summary:</h3>
<p>- The paper introduces ArcMMLU, a benchmark specifically designed for evaluating large language models (LLMs) in the Library & Information Science (LIS) domain in Chinese.<br>
- ArcMMLU covers four key subdomains: Archival Science, Data Science, Library Science, and Information Science. It includes over 6,000 high-quality questions that reflect the diverse nature of the LIS domain.<br>
- Mainstream LLMs achieve an average accuracy rate above 50% on ArcMMLU, but there is still room for improvement. Some models perform better with the introduction of few-shot examples, while others show a decline in performance.<br>
- GPT-4 demonstrates exceptional performance, outperforming other models in both zero-shot and five-shot settings. It performs particularly well in the Data Science subdomain.<br>
- Error analysis reveals that existing LLMs struggle with questions requiring specific domain knowledge and easily confusable names and concepts. Incorporating more comprehensive datasets and domain-specific training can enhance their performance.</p>
<a href="http://arxiv.org/pdf/2311.18658v1">Link</a>

<h2>Title: Prompt Optimization via Adversarial In-Context Learning</h2>
<h3>Summary:</h3>
<p>The paper proposes a new method called Adversarial In-Context Learning (adv-ICL) to optimize prompts for language models. It uses a generator and discriminator setup, similar to generative adversarial networks (GANs), where the generator tries to generate realistic output and the discriminator tries to distinguish between real and generated data. The prompts are updated in an adversarial manner using a prompt modifier.<br>
adv-ICL outperforms state-of-the-art prompt optimization techniques on various tasks including text summarization, machine translation, data-to-text generation, and classification. It is computationally efficient, easy to extend to different models and tasks, and effective in low-resource settings.<br>
The paper analyzes the theoretical aspects of adv-ICL and shows that it can achieve the desired equilibrium between the generator and discriminator. It also includes an ablation study to demonstrate the importance of optimizing both task instructions and demonstrations.<br>
The results of adv-ICL on different tasks show significant improvements over baselines, especially in generation and classification tasks. The method is also evaluated on MMLU and BIG-bench Hard benchmarks, achieving notable performance gains.<br>
The paper also includes an ablation study and qualitative analysis to further understand the effectiveness of adv-ICL and its design choices.</p>
<a href="http://arxiv.org/pdf/2312.02614v1">Link</a>

<h2>Title: Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games</h2>
<h3>Summary:</h3>
<p>- The study explores the application of Large Language Models (LLMs) in "Jubensha" games, a Chinese murder mystery role-playing game.<br>
- The researchers created a Chinese dataset specifically for Jubensha games, providing character scripts and game rules for AI agent development.<br>
- They designed a multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in the game and enhance gameplay dynamics.<br>
- The researchers evaluated the AI agents' mastery of case information and reasoning skills, and incorporated in-context learning to improve their performance.<br>
- The experimental results validate the effectiveness of their methods and provide insights into the capabilities of LLM-based agents in complex narrative environments.</p>
<a href="http://arxiv.org/pdf/2312.00746v1">Link</a>

<h2>Title: On the Effectiveness of Large Language Models in Domain-Specific Code Generation</h2>
<h3>Summary:</h3>
<p>- Large language models (LLMs) like ChatGPT have shown impressive capabilities in code generation, but their effectiveness in specific domains, such as web development and game development, needs further evaluation.<br>
- LLMs exhibit sub-optimal performance in generating domain-specific code due to their limited proficiency in utilizing domain-specific libraries.<br>
- Incorporating API knowledge as prompts can empower LLMs to generate more professional code in specific domains.<br>
- Strategies like external knowledge inquiring and chain-of-thought prompting can improve the effectiveness of domain-specific code generation.<br>
- Fine-tuning LLMs with domain-specific knowledge can further enhance their performance in code generation tasks.</p>
<a href="http://arxiv.org/pdf/2312.01639v1">Link</a>

<h2>Title: Boosting legal case retrieval by query content selection with large language models</h2>
<h3>Summary:</h3>
<p>- The paper focuses on enhancing legal case retrieval by utilizing salient content in legal case queries.<br>
- Traditional sparse and dense retrieval models do not correlate well with human annotations of salient content in queries.<br>
- Reformulating queries using large language models (LLMs) improves the performance of both sparse and dense models in legal case retrieval.<br>
- Different query reformulation methods (keyword extraction, key sentence extraction, and summary) have varying impacts on retrieval models.<br>
- Reformulated queries that highlight salient content can significantly improve retrieval performance compared to the original query.</p>
<a href="http://arxiv.org/pdf/2312.03494v1">Link</a>

<h2>Title: CLAMP: Contrastive LAnguage Model Prompt-tuning</h2>
<h3>Summary:</h3>
<p>- Large language models (LLMs) can be adapted for image classification by fine-tuning them using a contrastive image-caption matching objective.<br>
- Multimodal LLMs, which are tuned for generative tasks, underperform specialized models like CLIP in zero-shot image classification.<br>
- CLAMP (Contrastive LAnguage Model Prompt-tuning) is a method that replaces the text encoder of a multimodal LLM with a pretrained LLM and updates a limited set of parameters using a contrastive loss.<br>
- CLAMP achieves good image classification performance, outperforming state-of-the-art multimodal LLMs by 13%.<br>
- LLM initialization is particularly helpful for classification in domains that are under-represented in the visual pre-training data.</p>
<a href="http://arxiv.org/pdf/2312.01629v1">Link</a>

</body>
</html>