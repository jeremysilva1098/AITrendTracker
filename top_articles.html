---
layout: default
---
<style>
    body {
        zoom: 125%; /* Adjust the zoom level as per your requirement */
    }
</style>
<!DOCTYPE html>
<html>
<head>
    <title>Top Research Articles of the Week</title>
</head>
<body>
    <h1>Top Research Articles of the Week</h1>

    <h2>Title: Towards Safe and Aligned Large Language Models for Medicine</h2>
    <h3>Summary:</h3>
    <ul>
        <li>The paper discusses the importance of evaluating the safety and alignment of large language models (LLMs) in the medical field.</li>
        <li>The authors define medical safety and alignment based on the Principles of Medical Ethics set forth by the American Medical Association.</li>
        <li>They develop a dataset of harmful medical prompts to evaluate the safety and alignment of LLMs, and find that aligned LLMs have lower harmfulness scores compared to non-aligned LLMs.</li>
        <li>Medical LLMs tend to have higher harmfulness scores compared to general-knowledge LLMs, indicating a higher risk of harmful outputs.</li>
        <li>The paper highlights the need for further research and mitigation strategies to minimize the risks of harm of LLMs in medicine.</li>
    </ul>
    <a href="http://arxiv.org/pdf/2403.03744v1">Link</a>

    <h2>Title: Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications</h2>
    <h3>Summary:</h3>
    <ul>
        <li>This research introduces Morris II, the first worm designed to target GenAI ecosystems through the use of adversarial self-replicating prompts.</li>
        <li>The worm is capable of inserting prompts into inputs that, when processed by GenAI models, replicate the input as output and engage in malicious activities.</li>
        <li>Morris II has been tested against GenAI-powered email assistants and demonstrated the ability to perform malicious activities such as spamming and exfiltrating personal data.</li>
        <li>The performance of the worm is influenced by factors such as propagation rate, replication, and malicious activity.</li>
        <li>This research highlights the need for security measures to protect GenAI-powered applications and ecosystems from potential cyber attacks.</li>
    </ul>
    <a href="http://arxiv.org/pdf/2403.02817v1">Link</a>

    <h2>Title: Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment</h2>
    <h3>Summary:</h3>
    <ul>
        <li>The paper proposes a new prompting method called Causal Prompting to debias large language models (LLMs).</li>
        <li>Traditional debiasing methods focus on model training, but Causal Prompting addresses biases in LLMs by using causal inference and front-door adjustment.</li>
        <li>Causal Prompting uses chain-of-thoughts generated by LLMs as mediator variables and calculates the causal effect between input prompts and output answers.</li>
        <li>Contrastive learning is used to align the representation space of the encoder with LLMs for more accurate estimation of causal effects.</li>
        <li>Experimental results show that Causal Prompting achieves excellent performance on multiple natural language processing datasets.</li>
    </ul>
    <a href="http://arxiv.org/pdf/2403.02738v1">Link</a>

    <h2>Title: Reliable, Adaptable, and Attributable Language Models with Retrieval</h2>
    <h3>Summary:</h3>
    <ul>
        <li>Parametric language models (LMs) trained on large-scale web data face challenges such as factual errors, difficulty in verification, and adaptability.</li>
        <li>Retrieval-augmented LMs, which incorporate large-scale datastores during inference, can be more reliable, adaptable, and attributable.</li>
        <li>However, current retrieval-augmented LMs struggle with leveraging helpful text beyond knowledge-intensive tasks, have limited interaction between retrieval and LM components, and lack infrastructure for scaling.</li>
        <li>To address these challenges, a roadmap is proposed that includes rethinking datastores and retrievers, improving retriever-LM interactions, and investing in infrastructure for efficient training and inference.</li>
        <li>The adoption of retrieval-augmented LMs is limited compared to parametric LMs, and there is a need for advancements in architecture, training methodologies, and infrastructure to overcome these limitations.</li>
    </ul>
    <a href="http://arxiv.org/pdf/2403.03187v1">Link</a>

    <h2>Title: An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers</h2>
    <h3>Summary:</h3>
    <ul>
        <li>The study examines the performance of judge models that are fine-tuned on open-source language models for evaluating the quality of other language models.</li>
        <li>The findings show that while the fine-tuned judge models achieve high accuracy on specific test sets, they are inherently task-specific classifiers and their generalizability and fairness are lower compared to proprietary models like GPT4.</li>
        <li>The study also reveals that the fine-tuned judge models are overfitted to specific evaluation schemes and are biased towards superficial quality.</li>
        <li>It concludes that fine-tuned judge models should be used cautiously and are not a general substitute for GPT4 in LLM evaluation.</li>
        <li>The study suggests that the bias in LLM-based evaluators may come from the casual language modeling process, and alternative models like DeBERTa may offer better fairness in evaluation.</li>
    </ul>
    <a href="http://arxiv.org/pdf/2403.02839v1">Link</a>

    <h2>Title: Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems</h2>
    <h3>Summary:</h3>
    <ul>
        <li>Researchers have studied the scaling properties of compound inference systems that use multiple Large Language Model (LLM) calls and aggregate their responses.</li>
        <li>They found that the performance of one-layer Voting Inference Systems, which aggregate LLM responses via majority voting, initially increases but then decreases as the number of LLM calls increases.</li>
        <li>This non-monotonic behavior is due to the diversity of query difficulties within a task, with more LLM calls leading to higher performance on "easy" queries but lower performance on "hard" queries.</li>
        <li>The researchers derived a scaling law that models item difficulties and allows users to compute the optimal number of LLM calls for maximum system performance.</li>
        <li>They also demonstrated that their scaling law accurately predicts the performance of Voting Inference Systems and can be used to find the optimal number of LLM calls.</li>
    </ul>
    <a href="http://arxiv.org/pdf/2403.02419v1">Link</a>
</body>
</html>