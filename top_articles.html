---
layout: default
---
<style>
    body {
        zoom: 125%; /* Adjust the zoom level as per your requirement */
    }
</style>
<!DOCTYPE html>
<html>
<head>
<title>Top Research Articles of the Week</title>
</head>
<body>
<h1>Top Research Articles of the Week</h1>
<h2>Title: E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models</h2>
<h3>Summary:</h3>
<ul>
<li>The E-EVAL benchmark is introduced as the first comprehensive evaluation benchmark specifically designed for the Chinese K-12 education field.</li>
<li>Chinese-dominant models perform well compared to English-dominant models, with some models even outperforming GPT 4.0.</li>
<li>Models struggle with complex subjects such as mathematics, indicating the need for improvement in LLMs in this area.</li>
<li>Chinese-dominant LLMs do not necessarily perform better at the primary school level compared to the middle school level, showing that mastery of higher-order knowledge does not imply mastery of lower-order knowledge.</li>
<li>The Chain of Thought (CoT) technique is effective for challenging science subjects, while Few-shot prompting is more beneficial for liberal arts subjects.</li>
</ul>
<p>Link: <a href="http://arxiv.org/pdf/2401.15927v1">http://arxiv.org/pdf/2401.15927v1</a></p>

<h2>Title: Towards Efficient and Reliable LLM Serving: A Real-World Workload Study</h2>
<h3>Summary:</h3>
<ul>
<li>Large Language Models (LLMs), such as GPT models, have advanced significantly in recent years, but their development and deployment face challenges due to high operational costs.</li>
<li>This paper introduces BurstGPT, a real-world trace dataset of LLM serving workloads, which provides insights into the characteristics of LLM serving workloads.</li>
<li>The analysis of BurstGPT reveals burstiness patterns, variations in request and response distributions, and high failure rates in LLM serving systems.</li>
<li>The BurstGPT dataset and benchmark suite enable performance evaluation of serving systems and can be used to optimize LLM workload management and resource allocation.</li>
<li>Understanding these patterns is valuable for optimizing LLM serving systems and improving their efficiency and reliability in real-time applications.</li>
</ul>
<p>Link: <a href="http://arxiv.org/pdf/2401.17644v1">http://arxiv.org/pdf/2401.17644v1</a></p>

<h2>Title: Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain</h2>
<h3>Summary:</h3>
<ul>
<li>The paper proposes a "human-centered" modeling scheme for collaborative agents in games to enhance the experience of human players.</li>
<li>They introduce the Reinforcement Learning from Human Gain (RLHG) approach, which aims to enhance the extent to which humans achieve their goals while maintaining agents' original abilities.</li>
<li>The RLHG approach involves two stages: estimating the primitive human performance and training the agent to learn effective human enhancement behaviors.</li>
<li>The authors evaluate the RLHG agent in the popular MOBA game, Honor of Kings, and show that it provides participants with a better gaming experience in both objective performance and subjective preference.</li>
<li>The RLHG agent effectively improves the performance of both high-level and general-level participants in achieving their goals, bringing general-level participants closer to the performance of high-level participants.</li>
</ul>
<p>Link: <a href="http://arxiv.org/pdf/2401.16444v1">http://arxiv.org/pdf/2401.16444v1</a></p>

<h2>Title: Knowledge-Aware Code Generation with Large Language Models</h2>
<h3>Summary:</h3>
<ul>
<li>6.3 RQ3: Impact of Shot Times and ChatGPT Settings</li>
<li>Performance under Different Shot Times. To investigate the impact of shot times on KareCoder's performance, we conducted experiments with different shot times (1-shot and 3-shot) on the CodeF Post2021-9 dataset. As shown in Table 3, KareCoder maintained a consistently high performance across all difficulty levels and metrics, regardless of the shot times. In fact, KareCoder with 1-shot achieved better results than KareCoder with 3-shot in terms of Pass@1, Pass@3, and Pass@5. This indicates that, in practice, KareCoder can achieve good performance with fewer shots, which reduces the input length and saves resources.</li>
<li>Performance under Different ChatGPT Settings. In our experiments, we used the default configuration for ChatGPT, with the temperature parameter set to 1 and the Top_p parameter also set to 1. We did not observe significant differences in performance when changing these parameters. However, it is worth noting that different settings may affect the quality and diversity of the generated prompts and code. Further exploration of different ChatGPT settings may yield better results in terms of both accuracy and variety of generated code.</li>
<li>Answer to RQ3: Our experiments indicate that KareCoder exhibits a consistently high performance regardless of the shot times and ChatGPT settings. KareCoder achieves good results with fewer shots, which reduces input length and saves resources. While the default ChatGPT settings work well, exploring different settings could potentially yield improvements in the quality and diversity of generated prompts and code.</li>
<li>6.4 RQ4: Generalizability of KareCoder</li>
<li>To test the generalizability of KareCoder, we conducted experiments on the first 500 problems from the APPS test set. As shown in Table 6, KareCoder achieved good performance on the Pass@1 metric, surpassing other methods. This demonstrates that KareCoder is not limited to specific datasets and can be applied to other programming problems as well. However, it is worth noting that the performance of KareCoder may vary depending on the specific problem and dataset characteristics.</li>
<li>Answer to RQ4: The experiments on the APPS dataset confirm that KareCoder is not limited to specific datasets and can achieve good performance on other programming problems as well. This demonstrates the generalizability of KareCoder beyond the CodeF dataset.</li>
<li>7 CONCLUSION</li>
<li>In this paper, we introduced KareCoder, a knowledge-aware code generation method that enhances the performance of large language models (LLMs) in handling novel programming problems. KareCoder utilizes a tailored Knowledge Library and prompts to guide the code generation process of LLMs. Our experiments on the CodeF and APPS datasets showed that KareCoder outperformed other methods, including direct code generation by LLMs, in terms of accuracy and pass rates. KareCoder achieved a relative improvement of up to 24.4% on the Pass@1 metric. We also found that the Knowledge Description format of the Knowledge Library yielded the best performance. Additionally, we observed that KareCoder can achieve good results with fewer shots and is not limited to specific datasets. Overall, KareCoder demonstrates the potential for enhancing the capabilities of LLMs in code generation tasks by integrating external knowledge. In the future, we plan to explore more refined Knowledge Libraries and investigate additional techniques to further improve the performance of KareCoder.</li>
</ul>
<p>Link: <a href="http://arxiv.org/pdf/2401.15940v3">http://arxiv.org/pdf/2401.15940v3</a></p>

<h2>Title: Corrective Retrieval Augmented Generation</h2>
<h3>Summary:</h3>
<ul>
<li>The paper introduces Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of text generation models.</li>
<li>CRAG incorporates a lightweight retrieval evaluator to assess the relevance and accuracy of retrieved documents, allowing for corrective actions.</li>
<li>The proposed method includes knowledge refinement, web searches, and a decompose-then-recompose algorithm to filter out irrelevant information and improve the utilization of retrieved knowledge.</li>
<li>Experimental results show that CRAG significantly improves the performance of retrieval-augmented generation approaches on various tasks.</li>
<li>CRAG is plug-and-play and can be seamlessly integrated into existing retrieval-augmented generation models.</li>
</ul>
<p>Link: <a href="http://arxiv.org/pdf/2401.15884v1">http://arxiv.org/pdf/2401.15884v1</a></p>

<h2>Title: Ocassionally Secure: A Comparative Analysis of Code Generation Assistants</h2>
<h3>Summary:</h3>
<ul>
<li>The study compared the code generation capabilities of four advanced language models (GPT-3.5, GPT-4, Bard, and Gemini) across nine tasks relevant to building an e-commerce website.</li>
<li>GPT-3.5 and GPT-4 consistently produced functional code for all tasks, while Bard and Gemini had lower rates of functional code generation.</li>
<li>Security vulnerabilities were identified in the generated code, with Bard and Gemini showing fewer vulnerabilities compared to GPT-3.5 and GPT-4.</li>
<li>Code complexity, measured by cyclomatic complexity, and lines of code were similar across the models, with Gemini producing slightly more complex code.</li>
<li>The security persona had a mixed impact on code quality, with GPT models showing improved security outcomes, but Gemini showing an increase in vulnerabilities when using the security persona.</li>
<li>Developers should consider the specific characteristics and limitations of each model when using them for code generation tasks.</li>
</ul>
<p>Link: <a href="http://arxiv.org/pdf/2402.00689v1">http://arxiv.org/pdf/2402.00689v1</a></p>

</body>
</html>