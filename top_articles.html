---
layout: default
---
<style>
    body {
        zoom: 125%; /* Adjust the zoom level as per your requirement */
    }
</style>
<!DOCTYPE html>
<html>
<head>
<title>Top Research Articles of the Week</title>
</head>
<body>

<h1>Top Research Articles of the Week</h1>

<h2>Title: A Comprehensive Evaluation of Quantization Strategies for Large Language Models</h2>
<h3>Summary:</h3>
<ul>
<li>Quantization techniques can reduce the number of bits needed for model weights or activations in large language models (LLMs) with minimal performance loss.</li>
<li>LLMs with 4-bit quantization can maintain performance comparable to non-quantized models across a range of benchmarks.</li>
<li>Perplexity can be used as a proxy metric for quantized LLMs on most benchmarks.</li>
<li>Isolating outlier weights is crucial for effective quantization to extreme levels (2 bits) without significant performance degradation.</li>
<li>Quantizing LLMs can reduce memory consumption but may slow down inference speed, requiring engineering effort and hardware support for practical deployment.</li>
</ul>
<a href="http://arxiv.org/pdf/2402.16775v1">Link</a>

<h2>Title: Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas</h2>
<h3>Summary:</h3>
<ul>
<li>This survey examines cooperation in social dilemmas from the perspective of AI and highlights the advancements made in understanding and enhancing cooperation.</li>
<li>The survey focuses on three key areas: multi-agent cooperation, human-agent cooperation, and leveraging AI agents to enhance cooperation among humans.</li>
<li>In multi-agent cooperation, strategies are developed to promote cooperation among rational agents, including embedding human-like motives and using mechanisms like peer rewarding and formal contracts.</li>
<li>In human-agent cooperation, AI algorithms are designed to cooperate with humans, and human biases towards AI agents are studied to mitigate their impact on cooperation.</li>
<li>The survey also explores how AI agents can promote cooperation among humans by providing recommendations and shows the potential of AI agents in enhancing human-human cooperation in social dilemmas.</li>
</ul>
<a href="http://arxiv.org/pdf/2402.17270v1">Link</a>

<h2>Title: LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language</h2>
<h3>Summary:</h3>
<ul>
<li>The paper introduces LangGPT, a dual-layer prompt design framework for LLMs that is inspired by structured reusable programming languages.</li>
<li>LangGPT significantly enhances the capacity of LLMs to produce high-quality responses compared to baseline prompts.</li>
<li>LangGPT provides an easy-to-learn normative structure and supports migration and reuse of prompts.</li>
<li>The paper proposes prompt design rules based on the differences between programming languages and natural languages.</li>
<li>LangGPT has been evaluated through experiments and a user survey, demonstrating its effectiveness and ease of use.</li>
</ul>
<a href="http://arxiv.org/pdf/2402.16929v1">Link</a>

<h2>Title: RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering</h2>
<h3>Summary:</h3>
<ul>
<li>The paper introduces a benchmark dataset called RetrievalQA for evaluating adaptive retrieval-augmented generation (ARAG) methods in open-domain question answering.</li>
<li>The effectiveness of existing ARAG methods, including calibration-based and model-based approaches, is evaluated on RetrievalQA. Calibration-based methods require threshold tuning, while vanilla prompting is insufficient in guiding language models to make reliable retrieval decisions.</li>
<li>The paper proposes a new method called Time-Aware Adaptive Retrieval (TA-ARE) that improves ARAG without calibration or additional training. TA-ARE includes time awareness in the instruction and uses in-context demonstrations to enhance models' retrieval decisions.</li>
<li>TA-ARE significantly improves the retrieval and match accuracy of existing models on RetrievalQA, demonstrating the effectiveness of the proposed method.</li>
<li>The paper also provides error analysis and ablation studies to validate the performance of TA-ARE and highlight the challenges and potential improvements in ARAG for question answering tasks.</li>
</ul>
<a href="http://arxiv.org/pdf/2402.16457v1">Link</a>

<h2>Title: Pandora's White-Box: Increased Training Data Leakage in Open LLMs</h2>
<h3>Summary:</h3>
<ul>
<li>The paper presents membership inference attacks (MIAs) against pre-trained Large Language Models (LLMs) that can achieve high true positive rates and low false positive rates.</li>
<li>The authors propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single-step loss ratio attack. These attacks outperform existing black-box baselines and close the gap between MIA attack success against LLMs and other models.</li>
<li>In the fine-tuning setting, the authors find that a fine-tuned loss ratio attack (FLoRa) can achieve near-perfect MIA performance given access to the loss of the fine-tuned and base models.</li>
<li>The authors also show that using these MIAs, significant portions of the fine-tuning dataset can be extracted from fine-tuned LLMs, even after just a few epochs of fine-tuning.</li>
<li>The results highlight the vulnerability of LLMs to privacy attacks and the need for careful consideration before fine-tuning LLMs on sensitive data.</li>
</ul>
<a href="http://arxiv.org/pdf/2402.17012v1">Link</a>

</body>
</html>