---
layout: default
---
<style>
    body {
        zoom: 125%; /* Adjust the zoom level as per your requirement */
    }
</style>
<!DOCTYPE html>
<html>
<head>
<title>Top Research Articles of the Week</title>
</head>
<body>
<h1>Top Research Articles of the Week</h1>
<h2>Efficiently Programming Large Language Models using SGLang</h2>
<h3>Summary:</h3>
<ul>
<li>In this paper, the authors introduce SGLang, a domain-specific language for efficiently programming large language models (LLMs).</li>
<li>SGLang incorporates primitives for common LLM programming patterns and enables optimizations such as parallelism, batching, caching, and sharing.</li>
<li>The authors also propose RadixAttention, a technique for automatic key-value (KV) cache reuse across multiple generation calls at runtime.</li>
<li>The experiments show that SGLang can speed up common LLM tasks by up to 5x while reducing code complexity and enhancing control.</li>
</ul>
<h3>Link:</h3>
<a href="http://arxiv.org/pdf/2312.07104v1">http://arxiv.org/pdf/2312.07104v1</a>
<h2>Context Tuning for Retrieval Augmented Generation</h2>
<h3>Summary:</h3>
<ul>
<li>The paper introduces Context Tuning for Retrieval Augmented Generation (RAG), which improves tool retrieval and plan generation in large language models (LLMs).</li>
<li>Context Tuning uses a smart context retrieval system to fetch relevant information using numerical, categorical, and habitual usage signals.</li>
<li>Empirical results show that Context Tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool retrieval tasks respectively.</li>
<li>The proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART outperforms GPT-4 based retrieval.</li>
<li>Context augmentation at plan generation reduces hallucination in LLM-based planners.</li>
</ul>
<h3>Link:</h3>
<a href="http://arxiv.org/pdf/2312.05708v1">http://arxiv.org/pdf/2312.05708v1</a>
</body>
</html>